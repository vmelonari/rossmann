{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#lets import the training and test files:\ntrain_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")\nstore_df = pd.read_csv(\"../input/store.csv\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c4a406570edfd26b4412174c11b0a4481fcad33f"
      },
      "cell_type": "code",
      "source": "#lets see how many datas are in the files:\nprint(\"in the training set we have\", train_df.shape[0], \"observations and\", train_df.shape[1], \"columns/variables.\")\nprint(\"in the testing set we have\", test_df.shape[0], \"observations and\", test_df.shape[1], \"columns/variables.\")\nprint(\"in the store set we have\", store_df.shape[0], \"observations and\", store_df.shape[1], \"columns/variables.\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4b4dcffd9a94367bbc664a4eaa5dd46ee5bbe4b0"
      },
      "cell_type": "markdown",
      "source": "# Lets start first with understanding and cleaning the **training dataset.**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "db2b7c3ae6ce4d0c56e5fd93fd711669ca323030"
      },
      "cell_type": "code",
      "source": "#how does the data looks like:\ntrain_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4c59cfc23429452d056313c2c2c9257b3e9f4fd0"
      },
      "cell_type": "code",
      "source": "train_df.tail()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "636befb0dcf19621aab129a6a0debc3dbde9b21f"
      },
      "cell_type": "code",
      "source": "#wow. no missing values.\ntrain_df.isnull().all()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1df99d4dceb45aae3ccaf20f48bf53608102c4c2"
      },
      "cell_type": "markdown",
      "source": "Lets start with the first variable -> **Sales**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2e275db485dece9da6049298052bac51ff95376e"
      },
      "cell_type": "code",
      "source": "opened_sales = (train_df[(train_df.Open == 1) & (train_df.Sales)]) #if the stores are opend\nopened_sales.Sales.describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c3f387d8d06e194923c047d2b64192664fe0a9e8"
      },
      "cell_type": "code",
      "source": "f, ax = plt.subplots(1,2, figsize = (20, 5))\n\nopened_sales.Sales.plot(kind = \"hist\", title = \"Sales Histogram\", bins = 20, ax = ax[0])\nopened_sales.Sales.plot.box(title = \"Sales Boxplot\", ax = ax[1])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a372759bd9f36473c342a8277324a8895872827d"
      },
      "cell_type": "code",
      "source": "print(\"Rossmann has\", round(opened_sales.Sales[(opened_sales.Sales > 10000)].count() / opened_sales.shape[0] * 100, 2), \n      \"% of the time big sales, over 10.000 Euros\")\nprint(\"Rossmann has\", round(opened_sales.Sales[(opened_sales.Sales < 1000)].count() / opened_sales.shape[0] * 100, 4), \n      \"% of the time low sales, under 1000 Euros\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "000930b492a17dbe72d4723065cf0dd2f2ab9ae6"
      },
      "cell_type": "markdown",
      "source": "lets look at the **customers**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "513be872cf225adc333dfd58bbcf9ff4459ad072"
      },
      "cell_type": "code",
      "source": "train_df.Customers.describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b2432fb4f1a8035ff0bcbf18856b399980b62354"
      },
      "cell_type": "code",
      "source": "f, ax = plt.subplots(1,2, figsize = (20, 5))\n\ntrain_df.Customers.plot(kind = \"hist\", title = \"Customers Histogram\", bins = 20, ax = ax[0])\ntrain_df.Customers.plot.box(title = \"Sales Boxplot\", ax = ax[1])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f0f5121de1f52bd631a5e9ebd0399989270e404f"
      },
      "cell_type": "code",
      "source": "#Seems to had a great sortiment on 22th of January 2013. They hit the record of customers. \ntrain_df[(train_df.Customers > 6000)]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "100351a9cf70fa7a3c374860f7084a62b45121b4"
      },
      "cell_type": "markdown",
      "source": "Lets look at the **open** variable"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6a11fe5471a418341e74266af85e0af0dd76a0a0"
      },
      "cell_type": "code",
      "source": "print(\"In 3 years, different stores where\", train_df[(train_df.Open == 0)].count()[0], \"times closed\")\nprint(\"From this days,\", train_df[(train_df.Open == 0) & \n         ((train_df.StateHoliday == \"a\") | \n         (train_df.StateHoliday == \"b\") | \n         (train_df.StateHoliday == \"c\"))].count()[0], \"times the stores were closed because of holidays\")\nprint(train_df[(train_df.Open == 0) & (train_df.SchoolHoliday == 1)].count()[0], \"times, some stores were closed because of school holiday\")\nprint(\"The stores were in some sundays opend ->\", train_df[(train_df.Open == 1) & (train_df.DayOfWeek == 7)].count()[0], \"times\")\nprint(\"However,\", train_df[(train_df.Open == 0) & ((train_df.StateHoliday == \"0\") | (train_df.StateHoliday == 0)) & (train_df.SchoolHoliday == 0)].count()[0], \n      \"times, the stores were closed for no reason (No Holidays o Sunday)\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0fe2b7b909ffb4f68ddcea236fcc411f9a436db9"
      },
      "cell_type": "code",
      "source": "print(\"\"\"Rossman described clearly, that they were undergoing refurbishments sometimes and had to close. \n      Most probably those were the times this event was happening. \n      However, we dont want to have those obsvervations in our dataset, when predicting. So lets delete those days after we finished \n      our analysis\"\"\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "16431627591fe7d38ccd81c1352c9a2306f051cb"
      },
      "cell_type": "markdown",
      "source": "What about **Promotion**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5feaf366107ebea371dc32978e23f22dedd5f959"
      },
      "cell_type": "code",
      "source": "print(round((train_df.Promo[train_df.Promo == 1].count() / train_df.shape[0] * 100), 2), \"% of the time, has been promotions made\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3cc202ca339c5ad89a0868f309a2ff0cff37b770"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1c29c5c4391aaa650caa82614ad260b6bc1bb6e4"
      },
      "cell_type": "markdown",
      "source": "Now lets take a look at **StateHoliday** Variable."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4167e336ef839e8b1de9386d536bb36aefa4c182"
      },
      "cell_type": "code",
      "source": "# StateHoliday is not a continous number. \n# It is a string and I think not so important to know what kind of holiday (a, b or c). I will update it.\ntrain_df.StateHoliday.value_counts()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "641861a3b71a929a1a88de7427df83c8da25b838"
      },
      "cell_type": "code",
      "source": "train_df[\"StateHoliday_cat\"] = train_df[\"StateHoliday\"].map({0:0, \"0\": 0, \"a\": 1, \"b\": 1, \"c\": 1})\n\ntrain_df.StateHoliday_cat.count()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "627add449ca57a16ac471a659afb9273a2c6ed62"
      },
      "cell_type": "code",
      "source": "# let get rid of the StateHoliday column and use only the new one\ntrain_df = train_df.drop(\"StateHoliday\", axis = 1)\ntrain_df.tail()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ab4b778644760df04bd4ce5af37e6d4b200f57dd"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f420700e5e3750be54e2a1658a783e1d6b831ce2"
      },
      "cell_type": "code",
      "source": "#lets delete the times, where the stores were opened with no sales because of days in inventory.\ntrain_df = train_df.drop(train_df[(train_df.Open == 0) & (train_df.Sales == 0)].index)\ntrain_df = train_df.reset_index(drop = True) # to ge the indexes back to 0, 1, 2,etc.\n\ntrain_df.isnull().all() #to check for NaNs",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3353b9c3ef765a3630032374572629b8d126ca76"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "35c14832a25fb96bcfae6010354ff6f0abcfa46d"
      },
      "cell_type": "markdown",
      "source": "# Let's go ahead with the **store** analysis"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "456b7f5e7f38f0603e48b18924747595ab89c410"
      },
      "cell_type": "code",
      "source": "store_df.head(10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "93d1b014401d3c93427b7151961ff051c96b0599"
      },
      "cell_type": "code",
      "source": "#how may missing data do we have in %:\n100- (store_df.count() / store_df.shape[0] * 100)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8505e033d9c2a955b5c8a42a4041d101a6438e43"
      },
      "cell_type": "code",
      "source": "store_df.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "70c23dba395a9b729e65a47aea412ee84877315f"
      },
      "cell_type": "markdown",
      "source": "Lets start with the missing data. The first one is **CompetitionDistance**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0303dec8c7c1b879c2acdd8ff7be65dad75a6367"
      },
      "cell_type": "code",
      "source": "store_df.CompetitionDistance.plot.box() #let me see the outliers, so we can choose between mean and median to fill the NaNs\nprint(\"the median is\", store_df.CompetitionDistance.median(), \"and mean is\", store_df.CompetitionDistance.mean())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a94d84e443278406ae39b21afe6343e74e133492"
      },
      "cell_type": "code",
      "source": "print(\"Since we have here some outlier, its better to input the median value to those few missing values.\")\nstore_df[\"CompetitionDistance\"].fillna(store_df[\"CompetitionDistance\"].median(), inplace = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2af992d0ceec9b946195b8c9590154efec18af43"
      },
      "cell_type": "markdown",
      "source": "Next. What about **CompetitionOpenSinceMonth**  and **CompetitionOpenSinceYear**."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "73833af0932ed5f7404c6277cd4e57021532c1a2"
      },
      "cell_type": "code",
      "source": "#The missing values, are not there, because the stores had no competition. So I would suggest to fill the missing values with zeros.\nstore_df[\"CompetitionOpenSinceMonth\"].fillna(0, inplace = True)\nstore_df[\"CompetitionOpenSinceYear\"].fillna(0, inplace = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "15d5c03e1645176d6412ed99866959eba57cfcdd"
      },
      "cell_type": "markdown",
      "source": "Neeeext. Lets look at the promotions since Week and year and interval."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "be7c63d6c0ffb3fdf3bc7a3aa368cb96b3b4a0b4"
      },
      "cell_type": "code",
      "source": "store_df.groupby(by = \"Promo2\", axis = 0).count() \n# so if no promo has been made, then we should replace the NaN from Promo since Week and Year with zero",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ff1b653cb096e42f7ac3a52ce30c73bb4f8122eb"
      },
      "cell_type": "code",
      "source": "store_df[\"Promo2SinceWeek\"].fillna(0, inplace = True)\nstore_df[\"Promo2SinceYear\"].fillna(0, inplace = True)\nstore_df[\"PromoInterval\"].fillna(0, inplace = True)\n\nstore_df.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d1c440af75097c0a9723642a9da7b6014902c2f5"
      },
      "cell_type": "markdown",
      "source": "# So far, so good. Now lets **merg the files: stores and training,** so we can go on with our analysis."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9779f6c9baa4188a4318d6320953328ce38bcd77"
      },
      "cell_type": "code",
      "source": "train_store_df = pd.merge(train_df, store_df, how = \"left\", on = \"Store\")\ntrain_store_df.info()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4185011df53f1bf7fe186675496315a5be92396b"
      },
      "cell_type": "markdown",
      "source": "First. I am curious about the** store types**. So lets compare the stores by Sales, Customers, etc.\n\nStore Typ vs: Nr. of Stores, Total Sales, Cst per Store, Avg Sales,  Avg Customers, Avg Spending per Customer"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2618204a6b376c332805875f32bc67f765b88ae2"
      },
      "cell_type": "code",
      "source": "train_store_df[\"Avg_Customer_Sales\"] = train_store_df.Sales / train_store_df.Customers\n# train_store_df[\"Avg_Customers_Store\"] = train_store_df.Customers / train_store_df.Store",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d2168ce1ca5f2baf05416a4599bbf70716fed4fd"
      },
      "cell_type": "code",
      "source": "f, ax = plt.subplots(2, 3, figsize = (20,10))\n\nstore_df.groupby(\"StoreType\")[\"Store\"].count().plot(kind = \"bar\", ax = ax[0, 0], title = \"Total StoreTypes in the Dataset\")\ntrain_store_df.groupby(\"StoreType\")[\"Sales\"].sum().plot(kind = \"bar\", ax = ax[0,1], title = \"Total Sales of the StoreTypes\")\ntrain_store_df.groupby(\"StoreType\")[\"Customers\"].sum().plot(kind = \"bar\", ax = ax[0,2], title = \"Total nr Customers of the StoreTypes\")\ntrain_store_df.groupby(\"StoreType\")[\"Sales\"].mean().plot(kind = \"bar\", ax = ax[1,0], title = \"Average Sales of StoreTypes\")\ntrain_store_df.groupby(\"StoreType\")[\"Avg_Customer_Sales\"].mean().plot(kind = \"bar\", ax = ax[1,1], title = \"Average Spending per Customer\")\ntrain_store_df.groupby(\"StoreType\")[\"Customers\"].mean().plot(kind = \"bar\", ax = ax[1,2], title = \"Average Customers per StoreType\")\n\nplt.subplots_adjust(hspace = 0.3)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bc58cfa486a2578eb42a72f108c28a9f0da2837b"
      },
      "cell_type": "markdown",
      "source": "As we can see from the graphs, the StoreType a has the most stores, sales and customers. \nHowever the StoreType b has the best averages spendings per customers, with only 17 stores of this type. "
    },
    {
      "metadata": {
        "_uuid": "4f1122cbe937dc15679bdab11a4f5f6151bc9546"
      },
      "cell_type": "markdown",
      "source": "Lets check the **Assortments**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3752b245049892516017d8a3aee71a66e9ed1a1d",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "sns.countplot(data = train_store_df, x = \"StoreType\", hue = \"Assortment\", order=[\"a\",\"b\",\"c\",\"d\"]) \nprint(\"\"\"So only the StoreType B has all assortments. I think thats why they are performing so good. Maybe this StoreType has more sales area.\n      The assortment C is a good one, because the StoreType D has the best average customer spending.\"\"\")\n\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8b48296fa0fee1efe092a954338dd3e17c81dfc3"
      },
      "cell_type": "markdown",
      "source": "Lets go ahead with the **promotions**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ab237befb4b3b49091218cb09eeaa218989172fd"
      },
      "cell_type": "code",
      "source": "train_store_df.Date = train_store_df.Date.astype(\"datetime64[ns]\")\n\ntrain_store_df[\"Month\"] = train_store_df.Date.dt.month\ntrain_store_df[\"Year\"] = train_store_df.Date.dt.year\ntrain_store_df[\"Day\"] = train_store_df.Date.dt.day",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e89027c0dde22f8144afb12bd4e9e9e71ba47841"
      },
      "cell_type": "code",
      "source": "sns.factorplot(data = train_store_df, x =\"Month\", y = \"Sales\", \n               col = 'Promo', # per store type in cols\n               hue = 'Promo2',\n               row = \"Year\"\n             )\n# So, of course, if the stores are having promotion the sells are higher.\n# Overall the store promotions sellings are also higher than the seasionality promotions (Promo2). However I can't see no yearly trend. ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "87ad0d7b09f55e63b60fcfba93dee54ce827de13"
      },
      "cell_type": "raw",
      "source": ""
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "77915207e4471622894fe618519d17b3a1d45372",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "sns.factorplot(data = train_store_df, x = \"DayOfWeek\", y = \"Sales\", hue = \"Promo\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "59c66b49dbf477fe7764e7deb87ac61699bd1b0a"
      },
      "cell_type": "code",
      "source": "print(\"\"\"So, no promotion in the weekend. However, the sales are very high, if the stores have promotion. \nThe Sales are going crazy on Sunday. No wonder.\"\"\")\nprint(\"There are\", train_store_df[(train_store_df.Open == 1) & (train_store_df.DayOfWeek == 7)].Store.unique().shape[0], \"stores opend on sundays\")    \n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d6fafffb2b8ee0122dabaf0b3d66f045f467ec83"
      },
      "cell_type": "markdown",
      "source": "Let's see the trends on a yearly basis."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "235d9b25e4716573ca2081c3e42d918f2de16590"
      },
      "cell_type": "code",
      "source": "sns.factorplot(data = train_store_df, x = \"Month\", y = \"Sales\", col = \"Year\", hue = \"StoreType\")\n# Yes, we can see a seasonalities, but not trends. The sales stays constantly yearly. ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "162819c7c84ab48a7e3bdd332a28aa3f14b9485a"
      },
      "cell_type": "markdown",
      "source": "What about the **Competition Distance**. What kind of inpact does this have on the sales."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "26cd380419a3613aa0869b1b69873334c4e29d5c"
      },
      "cell_type": "code",
      "source": "train_store_df.CompetitionDistance.describe()\n# The obsverations are continous numbers, so we need to convert them into a categories. Lets a create a new variable.\ntrain_store_df[\"CompetitionDistance_Cat\"] = pd.cut(train_store_df[\"CompetitionDistance\"], 5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9e4a0ca14d979b5c74fd876e959496c82713dcd6"
      },
      "cell_type": "code",
      "source": "f, ax = plt.subplots(1,2, figsize = (15,5))\n\ntrain_store_df.groupby(by = \"CompetitionDistance_Cat\").Sales.mean().plot(kind = \"bar\", title = \"Average Total Sales by Competition Distance\", ax = ax[0])\ntrain_store_df.groupby(by = \"CompetitionDistance_Cat\").Customers.mean().plot(kind = \"bar\", title = \"Average Total Customers by Competition Distance\", ax = ax[1])\n\n# It is pretty clear. If the competions is very far away, the stores are performing better (sales and customers)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d7bc17aa105fc59bf84c7632b202022889ee87ba"
      },
      "cell_type": "markdown",
      "source": "**Lets go with the correlation graph.**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8fb3c424a8af7be5967ff7c68bd3fa3f7e2df9a8"
      },
      "cell_type": "code",
      "source": "# first we have to convert the variables to categories, bevor we convert them to codes.\n\n# train_store_df[\"Promo\"] = train_store_df[\"Promo\"].astype(\"category\") # it's already numerica\n# train_store_df[\"SchoolHoliday\"] = train_store_df[\"SchoolHoliday\"].astype(\"category\") # it's already numerica\ntrain_store_df[\"StoreType\"] = train_store_df[\"StoreType\"].astype(\"category\")\ntrain_store_df[\"Assortment\"] = train_store_df[\"Assortment\"].astype(\"category\")\n# train_store_df[\"Promo2\"] = train_store_df[\"Promo2\"].astype(\"category\") # it's already numerica\ntrain_store_df[\"PromoInterval\"] = train_store_df[\"PromoInterval\"].astype(\"category\")\n\ntrain_store_df[\"StoreType_cat\"] = train_store_df[\"StoreType\"].cat.codes\ntrain_store_df[\"Assortment_cat\"] = train_store_df[\"Assortment\"].cat.codes\ntrain_store_df[\"PromoInterval_cat\"] = train_store_df[\"Assortment\"].cat.codes\n\ntrain_store_df[\"StateHoliday_cat\"] = train_store_df[\"StateHoliday_cat\"].astype(\"float\")\ntrain_store_df[\"StoreType_cat\"] = train_store_df[\"StoreType_cat\"].astype(\"float\")\ntrain_store_df[\"Assortment_cat\"] = train_store_df[\"Assortment_cat\"].astype(\"float\")\ntrain_store_df[\"PromoInterval_cat\"] = train_store_df[\"PromoInterval_cat\"].astype(\"float\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "906fc48c06c9a31afe062ce3c74a40f765b7550d"
      },
      "cell_type": "code",
      "source": "train_store_df.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "63ec171a7ddc48d732d5ef5c836fbaf3c23cb7d0"
      },
      "cell_type": "code",
      "source": "df_correlation = train_store_df[[\"Store\", \"DayOfWeek\", \"Sales\", \"Customers\", \"Promo\", \"SchoolHoliday\", \"CompetitionDistance\", \n                                 \"CompetitionOpenSinceMonth\", \"CompetitionOpenSinceYear\", \"Promo2\", \"Promo2SinceWeek\", \"Avg_Customer_Sales\", \n                                 \"Month\", \"Year\", \"Day\", \"StateHoliday_cat\", \"Assortment_cat\", \"StoreType_cat\", \"PromoInterval_cat\"]]\n\n\nf, ax = plt.subplots(figsize = (15, 10))\nsns.heatmap(df_correlation.corr(),ax = ax, annot=True, cmap=sns.diverging_palette(10, 133, as_cmap=True), linewidths=0.5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dd251bffbff5df942a24f074f217eb21542ed699"
      },
      "cell_type": "markdown",
      "source": "Following correlations we can confirm from the graph above: \n* Customer vs Sales (0.82) \n* Promo vs Sales (0,82)\n* Avg_Customer_Sales vs Promo (0,28)\n* Avg_Customer_Sales vs Promo2 (0,22)\n* StoreTypve vs Avg_Customer_Sales (0,44)"
    },
    {
      "metadata": {
        "_uuid": "23c22a0a14cce9bb502774f527a044b109221cad"
      },
      "cell_type": "markdown",
      "source": "# My conclusion of the analysis: \nI am so curious about forecasting, trying it with different models. So, I wll come back later to write my conclusion..."
    },
    {
      "metadata": {
        "_uuid": "86f3b020659ac596425cb8096c4203bdad756020"
      },
      "cell_type": "markdown",
      "source": ""
    },
    {
      "metadata": {
        "_uuid": "a40d856fd7bf27a3d57bddf6486fd187129d03d9"
      },
      "cell_type": "markdown",
      "source": "# ARIMA Forecasting"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "438a3f90d65b44f79ff488a7ae0b0633e2bfc6af"
      },
      "cell_type": "code",
      "source": "ts_arima = train_store_df.set_index(\"Date\").resample(\"W\").mean() #set the index to date and resample it by summing to monthly values\nts_arima = ts_arima[[\"Sales\"]]\nts_arima.plot()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "44fa7a41a6aab52aeccbe1862663cf327453c952"
      },
      "cell_type": "code",
      "source": "# Les see if we have stationary or non-stationary time series\n\nfrom statsmodels.tsa.stattools import adfuller\ndef test_stationarity(timeseries):\n    \n    #Determing rolling statistics\n    rolmean = timeseries.rolling(window=12).mean()\n    rolstd = timeseries.rolling(window=12).std()\n\n    #Plot rolling statistics:\n    orig = plt.plot(timeseries, color='blue',label='Original')\n    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n    plt.legend(loc='best')\n    plt.title('Rolling Mean & Standard Deviation')\n    plt.show(block=False)\n    \n#     #Perform Dickey-Fuller test:\n#     print('Results of Dickey-Fuller Test:')\n#     dftest = adfuller(timeseries, autolag='AIC')\n#     dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n#     for key,value in dftest[4].items():\n#         dfoutput['Critical Value (%s)'%key] = value\n#     print (dfoutput)\n    \ntest_stationarity(ts_arima)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "039332cc05389d3ffe5830d0ae4b99f08a108a26"
      },
      "cell_type": "markdown",
      "source": "hmm.. **the script is dosent work well, so i cant check the critical value and test statistic.**\nFYI: if the test statistic is greater than the critcal value, then we can not reject the null hypothesis, the series is stationary. That said it is still non-stationary. If you increase the i value in ARIMA model, perhaps above condition may meet and you may get the good forecast values"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d8dc9789716dfb091a49e2a03643040eda38a43a"
      },
      "cell_type": "markdown",
      "source": "As we can see from the graph, we are dealing with a seasonal Time Series. So I need to find the right parameters for the ARIMA Model as ARIMA(p,d,q)(P,D,Q)s. Here (p,d,q) are the non-seasonal parameter, while (P,D,Q) follow the same defintion, but are for the seasonal component of the series. The term s, is the periodicity (4 for the quarerly periods, 12 for yearly periods, etc.)\n\n* p is the auto-regressive part of the model. It allows us to incorporate the effect of past values into our model. Intuitively, this would be similar to stating that it is likely to be warm tomorrow if it has been warm the past 3 days.\n* d is the integrated part of the model. This includes terms in the model that incorporate the amount of differencing (i.e. the number of past time points to subtract from the current value) to apply to the time series. Intuitively, this would be similar to stating that it is likely to be same temperature tomorrow if the difference in temperature in the last three days has been very small.\n* q is the moving average part of the model. This allows us to set the error of our model as a linear combination of the error values observed at previous time points in the past."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "be563849020b034e1a0cf2f53c56ea9793df952f"
      },
      "cell_type": "code",
      "source": "import warnings\nimport itertools\nimport statsmodels.api as sm\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\n# Let's begin by generating the various combination of parameters that we wish to assess:\n\n# Define the p, d and q parameters to take any value between 0 and 2\np = d = q = range(0, 2)\n\n# Generate all different combinations of p, q and q triplets\npdq = list(itertools.product(p, d, q))\n\n# Generate all different combinations of seasonal p, q and q triplets\nseasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\n\nprint('Examples of parameter combinations for Seasonal ARIMA...')\nprint('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))\nprint('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))\nprint('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))\nprint('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "01260432efcf01420a9fbef0b7ae36ac31ed5b9e"
      },
      "cell_type": "markdown",
      "source": "Lets iterate through some combinations of parameters and use the SARIMAX function to get the AIC Score. The lowes AIC value is the optimal option for our model"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "01dd79f1fb178e9681385dd7d454aae00bdaa900"
      },
      "cell_type": "code",
      "source": "warnings.filterwarnings(\"ignore\") # specify to ignore warning messages\n\nfor param in pdq:\n    for param_seasonal in seasonal_pdq:\n        try:\n            mod = sm.tsa.statespace.SARIMAX(ts,\n                                            order=param,\n                                            seasonal_order=param_seasonal,\n                                            enforce_stationarity=False,\n                                            enforce_invertibility=False)\n\n            results = mod.fit()\n\n            print('ARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, results.aic))\n        except:\n            continue",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "182c86a55e6726133c431cb84e715425571ed047"
      },
      "cell_type": "code",
      "source": "# this is the optimal paramater for our model: ARIMA(1, 1, 0)x(1, 1, 0, 12)12 - AIC:183.74279077423745\n\nmod = sm.tsa.statespace.SARIMAX(ts_arima,\n                                order=(1, 1, 1),\n                                seasonal_order=(1, 1, 1, 12),\n                                enforce_stationarity=False,\n                                enforce_invertibility=False)\n\nresults = mod.fit()\n\nprint(results.summary().tables[1])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1ff35275b1be7c9a08f1db3155917850823850fe"
      },
      "cell_type": "markdown",
      "source": "* Lets focus only on **coef**. This column shows the importance (weight) of each feature and how each one impacts the time series. \n* The **P>|z|** column informs us of the significance of each feature. Here, weach weight has a p-value higher then 0, \n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fe41753128a84ca75887a330807e796a431d6017"
      },
      "cell_type": "code",
      "source": "results.plot_diagnostics(figsize=(15, 12))\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5ea772809fea1695d43e4714f8d2b0c6cb2ac062"
      },
      "cell_type": "markdown",
      "source": "Our primary concern is to ensure that the residuals of our model are uncorrelated and normally distributed with zero-mean. If the seasonal ARIMA model does not satisfy these properties, it is a good indication that it can be further improved.\n\nIn this case, our model diagnostics suggests that the model residuals are normally distributed based on the following:\n\n* In the top right plot, we see that the red KDE line don't follows closely with the N(0,1) line (where N(0,1)) is the standard notation for a normal distribution with mean 0 and standard deviation of 1). This is a good indication that the residuals are normally distributed.\n* The qq-plot on the bottom left shows that the ordered distribution of residuals (blue dots) follows the linear trend of the samples taken from a standard normal distribution with N(0, 1). Again, this is a strong indication that the residuals are normally distributed.\n* The residuals over time (top left plot) don't display any obvious seasonality and appear to be white noise. This is confirmed by the autocorrelation (i.e. correlogram) plot on the bottom right, which shows that the time series residuals have low correlation with lagged versions of itself."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f00e2207d910902e0baff13373a034d9c01f3726"
      },
      "cell_type": "code",
      "source": "# Lets go ahead with validating forecasts\n\npred = results.get_prediction(start = pd.to_datetime(\"2015-01-11\"), dynamic = False) \n# lets start the fc to start from 1.11.2015. \n# The dynamic=False argument ensures that we produce one-step ahead forecasts, meaning that forecasts at each point are generated using the full history up to that point.\npred_ci = pred.conf_int() # Get confidence intervals of forecasts\n\nax = ts_arima[\"2014\":].plot(label = \"observed\", figsize=(15, 7))\npred.predicted_mean.plot(ax = ax, label = \"One-step ahead FC\", alpha = 1)\nax.fill_between(pred_ci.index, \n                pred_ci.iloc[:, 0], \n                pred_ci.iloc[:, 1], \n                color = \"k\", alpha = 0.05)\n\nax.set_xlabel(\"Date\")\nax.set_ylabel(\"Sales\")\n\nplt.legend\nplt.show()\n\n# -----------\n#  extract the predicated and true values of our time series\nts_forecasted = pred.predicted_mean\nts_truth = ts_arima[\"2015-01-11\":]\n# to use, in my case, the mean squared error:\nrms_arima = sqrt(mean_squared_error(ts_truth, ts_forecasted))\nprint(\"RMS:\", rms_arima) ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3a512014f3bb3ddf8a857153ca83f092ddbd32f7"
      },
      "cell_type": "code",
      "source": "# lets try a dynamic forecast. In this case we will use information from the time series up to a certain point, to generate future values.\npred_dynamic = results.get_prediction(start = pd.to_datetime(\"2015-01-11\"), dynamic = True, full_results = True)\npred_dynamic_ci = pred_dynamic.conf_int()\n\nax = ts_arima[\"2014\":].plot(label = \"observed\", figsize = (20, 7))\npred_dynamic.predicted_mean.plot(label =\"Dynamic Forecast\", ax = ax)\n\nax.fill_between(pred_dynamic_ci.index,\n               pred_dynamic_ci.iloc[:, 0],\n               pred_dynamic_ci.iloc[:, 1], color =\"k\", alpha = 0.25)\n\nax.fill_betweenx(ax.get_ylim(), pd.to_datetime(\"2015-01-11\"), ts_arima.index[-1],\n                alpha = 0.1, zorder =-1)\n\nax.set_xlabel(\"Date\")\nax.set_ylabel(\"Sales\")\n\nplt.legend()\nplt.show()\n\n# -----------\n#  extract the predicated and true values of our time series\nts_forecasted = pred_dynamic.predicted_mean\nts_truth = ts_arima[\"2015-01-11\":]\n# to use, in my case, the mean squared error:\nrms_arima_dynamic = sqrt(mean_squared_error(ts_truth, ts_forecasted))\nprint(\"RMS:\", rms_arima_dynamic) ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "95010bd6924a588a5914b77214f04be22749cd62"
      },
      "cell_type": "markdown",
      "source": "I dont know what am I doing here wrong, but the forecast is worster..."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7eab9c63b490bd39d18db7e60ec58edaaab3c9b6"
      },
      "cell_type": "code",
      "source": "pred_uc = results.get_forecast(steps = 60) # lets get a forecast for the next few periods\npred_ci = pred_uc.conf_int() # Get confidence intervals of forecasts\n\nax = ts_arima.plot(label = \"observed\", figsize = (20,7))\npred_uc.predicted_mean.plot(ax = ax, label = \"Forecast\")\nax.fill_between(pred_ci.index, \n               pred_ci.iloc[:, 0],\n               pred_ci.iloc[0, 1], color = \"k\", alpha = 0.25)\nax.set_xlabel(\"Date\")\nax.set_ylabel(\"Sales\")\n\nplt.legend()\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ff5bf661cd97d0f1f6bcb666bece0a6a68bfe0ca"
      },
      "cell_type": "markdown",
      "source": "**My ARIMA Forecast not so good. So lets use another regression forecast -> **"
    },
    {
      "metadata": {
        "_uuid": "ea462c8b8c20db17aa6e276cb519340da62c7936"
      },
      "cell_type": "markdown",
      "source": "# Prophet\nhttps://facebook.github.io/prophet/docs/quick_start.html#python-api"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "13a40d753dfc18664fae0efb16b6d7a8ceec2dc8"
      },
      "cell_type": "code",
      "source": "from fbprophet import Prophet",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bf9c2b6958400b41b47770f0a5a2bd8b052d8258"
      },
      "cell_type": "code",
      "source": "# I want to create a new dataframe for this model.\nts_prophet = train_store_df.drop(['Store', 'DayOfWeek', 'Customers', 'Open', 'Promo',\n       'StoreType', 'Assortment',\n       'CompetitionDistance', 'Promo2', 'Promo2SinceWeek', 'Promo2SinceYear',\n       'PromoInterval', 'Avg_Customer_Sales', 'Month', 'Year', 'Day',\n       'CompetitionDistance_Cat', 'StoreType_cat', 'Assortment_cat',\n       'PromoInterval_cat', \"CompetitionOpenSinceMonth\", \"CompetitionOpenSinceYear\"], axis = 1)\n\nts_prophet.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "161d8fa53db172a03b70115372255ba56f2529f0"
      },
      "cell_type": "code",
      "source": "# as I understand from the documentation, the variables should have a specific names\nts_prophet = ts_prophet.rename(columns = {\"Date\": \"ds\",\n                          \"Sales\": \"y\"})\n\nts_prophet.tail()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f370ee3e193f3b2c8c3cfb00eb9a9ed9951c9078"
      },
      "cell_type": "code",
      "source": "# In prophet we can also model the holidays. so lets go for it.\n\nstate_dates = ts_prophet[(ts_prophet.StateHoliday_cat == 1)].loc[:, \"ds\"].values\nschool_dates = ts_prophet[(ts_prophet.SchoolHoliday == 1)].loc[:, \"ds\"].values\n\nstate = pd.DataFrame({\"holiday\": \"state_holiday\", \n                     \"ds\": pd.to_datetime(state_dates)})\nschool = pd.DataFrame({\"holiday\": \"school_holiday\",\n                      \"ds\": pd.to_datetime(school_dates)})\n\nholidays = pd.concat((state, school))\nholidays.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "eedfd219421fa574e5d5ba4a1acecd51c6031b69"
      },
      "cell_type": "code",
      "source": "ts_prophet = ts_prophet.drop([\"SchoolHoliday\", \"StateHoliday_cat\"], axis = 1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "728b32cd61b296d230f4a01a653a57920dc4b302"
      },
      "cell_type": "code",
      "source": "# it takes just too long to fit the model with a daily time series, so lets make it weekly.\nts_week_prophet = ts_prophet.set_index(\"ds\").resample(\"W\").sum()\nts_week_prophet_train = ts_week_prophet[\"2013-01-01\": \"2015-01-11\"] #I will slice the dataframe, so we can have some testing data \nts_week_prophet_train = ts_week_prophet_train.reset_index()\nts_week_prophet = ts_week_prophet.reset_index() # here are all the weekly data\n\nholidays_week = holidays.set_index(\"ds\").resample(\"W\").min()\nholidays_week = holidays_week.dropna(axis = 0)\n# holidays_week.holiday.fillna(0, inplace = True)\nholidays_week = holidays_week.reset_index()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "78364cdd32db443a3aaed94ad0c2ab9524f4646a"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "302b86dbb2bbc4bc794a4bc89c4885a8416d7915"
      },
      "cell_type": "code",
      "source": "# help(Prophet)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "aa8edd82eca4c38d7af8c69f0d9f9b09b486e0e7"
      },
      "cell_type": "code",
      "source": "# lets fit the model\nprophet = Prophet() # holidays = holidays_week\n# prophet = Prophet(interval_width = 0.80, holidays = holidays, weekly_seasonality=True, daily_seasonality=False) # the default uncertainty is 80 %\nprophet.fit(ts_week_prophet_train)\nprint(\"done\")\n\nfuture = prophet.make_future_dataframe(periods = 52, freq = \"W\") # here we are extending our dataframe with the dates for which a prediction is to be made.\nforecast = prophet.predict(future) # with predict method I asign each row in future dates a predicted value, which it names yhat\n\nforecast[[\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]].tail() # We have a new dataframe, which includes, the forecast and the uncertainity invervals.\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cf5bbfb9af5539d83db00a04ad5c96dc1d9fd246"
      },
      "cell_type": "code",
      "source": "fig1 = prophet.plot(forecast) #plot the results for the forecast time.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d0e6db45e1709363674b76fe1eef188331d1a8a5"
      },
      "cell_type": "code",
      "source": "# with this method we can see the components (trend, yearly seasonality and weekly seasonality of the time series.).\nfig2 = prophet.plot_components(forecast)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7135b6760a841f7bcf51beabea7fd35035e1e2e5"
      },
      "cell_type": "markdown",
      "source": "**Plot forecast.yhat vs ts_prohet_week.ds** and then calculate the rms"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d560b6aa09230e56b4d97dad55b43beef3dbebdd"
      },
      "cell_type": "code",
      "source": "fc_week_prophet = forecast[[\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]] # create a df with only the importan variables",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4936a7846062d60e1a4d06ae3ab61cc10394f2af"
      },
      "cell_type": "code",
      "source": "fc_week_prophet = fc_week_prophet.merge(ts_week_prophet, how = \"left\", on = \"ds\") #add the original data to the fc frame, so we can compare \nfc_week_prophet = fc_week_prophet.set_index(\"ds\") # make a time series index",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d54c7494cafacd90cae3bc4940ff650ffb71c044"
      },
      "cell_type": "code",
      "source": "plt.figsize=(10,20)\nfc_week_prophet[\"y\"].plot(figsize=(17,10))\nfc_week_prophet[\"yhat\"].plot()\n\nplt.legend()\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1f27540a70b5b811b7286b99d2ea427bf437ed25"
      },
      "cell_type": "code",
      "source": "# y_prophet = fc_week_prophet[\"y\"][: \"2015-08-02\"]\n# yhat_prophet = fc_week_prophet[\"yhat\"][: \"2015-08-02\"]\n\n# rms_prophet = sqrt(mean_squared_error(fc_week_prophet[\"y\"], fc_week_prophet[\"yhat\"]))\n# print(\"RMS:\", rms_prophet) \n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c484335a2825ef17d09d1e63fbc78bfe3cfd99e2"
      },
      "cell_type": "markdown",
      "source": "# Random Forest Regression"
    },
    {
      "metadata": {
        "_uuid": "8f6b0b31a225e913c094177d019eadfaf98c4368"
      },
      "cell_type": "markdown",
      "source": "Before we can start with fit and train our model, we need to do some feature engineering. \n\nThe CompetitionOpen since...  variables have the same meaning. So lets convert them into one variable that we call CompetitionOpenSince. It makes it easier for the algorithm to understand the pattern and creates less branches and thus complex trees."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6f122a02087b023ed913a6cb24960cd2ba94fc6e"
      },
      "cell_type": "code",
      "source": "train_store_df[\"CompetitionOpenSince\"] = np.where((train_store_df[\"CompetitionOpenSinceMonth\"] == 0) & (train_store_df[\"CompetitionOpenSinceYear\"] == 0), \n                                                0,(train_store_df.Month - train_store_df.CompetitionOpenSinceMonth) + (12 *(train_store_df.Year - train_store_df.CompetitionOpenSinceYear)))\n\n# lets drop the variables\ntrain_store_df = train_store_df.drop([\"CompetitionOpenSinceMonth\", \"CompetitionOpenSinceYear\"], axis = 1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bafe13c00e24048434c2866a9164f799ff009026"
      },
      "cell_type": "code",
      "source": "# lets drop few variables, that either or not numeric or we dont need them anymore\n# lets create a new data frame for this model\nts_rfr = train_store_df.copy()\nts_rfr = train_store_df.drop([\"Date\",\"StoreType\", \"Assortment\", \"PromoInterval\", \"CompetitionDistance_Cat\"], axis = 1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2e473c06ca712b21b9d960748be3a30ef26bd1cb"
      },
      "cell_type": "markdown",
      "source": "**Let's go ahead and develop the Model**\n\nI will use now only the training Dataset. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "25ba4e8cff4f290cec88d8856ba2b0b614c1c490"
      },
      "cell_type": "code",
      "source": "from sklearn import model_selection\nfrom sklearn import metrics\n\nfeatures = ts_rfr.drop([\"Customers\", \"Sales\", \"Avg_Customer_Sales\"], axis = 1)\ntarget = ts_rfr[\"Sales\"]\n\nX_train, X_train_test, y_train, y_train_test = model_selection.train_test_split(features, target, test_size = 0.20, random_state = 15) \n# I call here train_test_set which is  divided 80% and 20% validation\nprint(X_train.shape, X_train_test.shape, y_train.shape, y_train_test.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1ae647104912394f194cc5d2df7b023bfb697b80"
      },
      "cell_type": "markdown",
      "source": "Lets try our **RandomForestRegressor**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2a176fc0b85ec2201bd73d8008bf4145fcf7a1de"
      },
      "cell_type": "code",
      "source": "# # Try different numbers of n_estimators - this will take a minute or so\n# from sklearn.ensemble import RandomForestRegressor\n\n# model = RandomForestRegressor(n_jobs=-1)\n# estimators = np.arange(10, 200, 10)\n# scores = []\n# for n in estimators:\n#     model.set_params(n_estimators=n)\n#     model.fit(X_train, y_train)\n#     scores.append(model.score(X_train_test, y_train_test))\n# plt.title(\"Effect of n_estimators\")\n# plt.xlabel(\"n_estimator\")\n# plt.ylabel(\"score\")\n# plt.plot(estimators, scores)\n\n# #another script that takes toooo long, to find the right parameters for RFR\n# params = {'max_depth':(4,6,8,10,12,14,16,20),\n#          'n_estimators':(4,8,16,24,48,72,96,128),\n#          'min_samples_split':(2,4,6,8,10)}\n# #scoring_fnc = metrics.make_scorer(rmspe)\n# #the dimensionality is high, the number of combinations we have to search is enormous, using RandomizedSearchCV \n# # is a better option then GridSearchCV\n# grid = model_selection.RandomizedSearchCV(estimator=rfr,param_distributions=params,cv=10) \n# #choosing 10 K-Folds makes sure i went through all of the data and didn't miss any pattern.(takes time to run but is worth doing it)\n# grid.fit(X_train, y_train)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9efd173a7fc620745faf0ef4177b644000e05b8d"
      },
      "cell_type": "code",
      "source": "from sklearn.ensemble import RandomForestRegressor\n\nrfr = RandomForestRegressor(n_estimators=10, \n#                              criterion='mse', \n#                              max_depth=5, \n#                              min_samples_split=2, \n#                              min_samples_leaf=1, \n#                              min_weight_fraction_leaf=0.0, \n#                              max_features='auto', \n#                              max_leaf_nodes=None, \n#                              min_impurity_decrease=0.0, \n#                              min_impurity_split=None, \n#                              bootstrap=True, \n#                              oob_score=False,\n#                              n_jobs=4,\n#                              random_state=31, \n#                              verbose=0, \n#                              warm_start=False\n                           )\nrfr.fit(X_train, y_train)\nyhat = rfr.predict(X_train_test)\nrms_rfr = sqrt(mean_squared_error(y_train_test, yhat))\nprint(\"RMS:\", rms_rfr) ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "66939fa0d6635e317e052cf67d7c863e6e91a7cb"
      },
      "cell_type": "markdown",
      "source": "In order to understand better what happened when we ran our randomforest regressor, here is a chart that represents, the importance and role that each variable that i decided to include played in this learning process:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5e10e0d6228aa18a9af6317be68d689788d8b520"
      },
      "cell_type": "code",
      "source": "importances = rfr.feature_importances_\nstd = np.std([rfr.feature_importances_ for tree in rfr.estimators_],\n             axis=0)\nindices = np.argsort(importances)\npalette1 = itertools.cycle(sns.color_palette())\n# Store the feature ranking\nfeatures_ranked=[]\nfor f in range(X_train.shape[1]):\n    features_ranked.append(X_train.columns[indices[f]])\n# Plot the feature importances of the forest\n\nplt.figure(figsize=(10,10))\nplt.title(\"Feature importances\")\nplt.barh(range(X_train.shape[1]), importances[indices],\n            color=[next(palette1)], align=\"center\")\nplt.yticks(range(X_train.shape[1]), features_ranked)\nplt.ylabel('Features')\nplt.ylim([-1, X_train.shape[1]])\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "761659a8c3dd96723d93940ad7aee4826f5374bb"
      },
      "cell_type": "markdown",
      "source": "**y_train vs y_train_test plot** "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8ee62286eb8efc2d84d907f74e6769df1e2674d3"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "041f0791cd082c5d7077bb228bde4fe24c702dbf"
      },
      "cell_type": "markdown",
      "source": "# XGBoost"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3b18cfb2b3e7fe9845455a8f19535935f80bc521"
      },
      "cell_type": "code",
      "source": "def rmspe(y, yhat):\n    return np.sqrt(np.mean((yhat/y-1) ** 2))\n\ndef rmspe_xg(yhat, y):\n    y = np.expm1(y.get_label())\n    yhat = np.expm1(yhat)\n    return \"rmspe\", rmspe(y,yhat)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bedd1ec44657f6c92d47c21a82bb4ba21b3decab"
      },
      "cell_type": "code",
      "source": "import xgboost as xgb\n\nparam = {'max_depth':10, # maximum depth of a tree\n         \"booster\": \"gbtree\",   # use tree based models \n         'eta':1, # learning rate\n         'silent':1, # silent mode\n         'objective':'reg:linear', # for linear regression\n#          \"seed\": 10,   # Random number seed\n#          \"subsample\": 0.9,    # Subsample ratio of the training instances\n        }\n\nnum_round = 100 #how many boosting rounds\n\ndtrain = xgb.DMatrix(X_train, y_train)\ndtest = xgb.DMatrix(X_train_test, y_train_test)\nwatchlist = [(dtrain, 'train'), (dtest, 'eval')]\n\nxgboost = xgb.train(param, dtrain, num_round, evals=watchlist, \\\n  early_stopping_rounds= 100, feval=rmspe_xg, verbose_eval=True)\n         \n# make prediction\npreds = xgboost.predict(dtest)\n\n# model = xgb.train(params, dtrain, num_boost_round, evals=watchlist, \\\n#   early_stopping_rounds= 100, feval=rmspe_xg, verbose_eval=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c6f55c3802cfab272d004ab0d1411d37bd08e93c"
      },
      "cell_type": "code",
      "source": "rms_xgboost = sqrt(mean_squared_error(y_train_test, preds))\nprint(\"RMS:\", rms_xgboost) ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "999ddbb46000181fe98498c4101494e3bfd8cbe2"
      },
      "cell_type": "code",
      "source": "# Lets see the feature importance\nfig, ax = plt.subplots(figsize=(10,10))\nxgb.plot_importance(xgboost, max_num_features=50, height=0.8, ax=ax)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e7caf6e18d1dc5a503d69b222f34d3cb10f462c6"
      },
      "cell_type": "code",
      "source": "print(\"We got the follwing Errors:\\n\",\n     \"-RMS Arima\", rms_arima ,\"\\n\"\n     \"-RMS Arima Dynamic\", rms_arima_dynamic ,\"\\n\"\n#      \"-RMS Prophet\", rms_prophet ,\"\\n\"\n     \"-RMS Random Forest Regression\", rms_rfr ,\"\\n\"\n     \"-XGBoost\", rms_xgboost ,\"\\n\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f504060370fca191116493296a0b708135cb5678"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e777fad3839ef45d1f25d50fdefe68d130c788d1"
      },
      "cell_type": "markdown",
      "source": "# Test Dataset, if want to play with it too"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "256f5895b4ba89cdd7f1140613169d4cada727c1"
      },
      "cell_type": "code",
      "source": "# # As we are working with a decision tree based model, we need to use dummy variables instead of categorical levels. \n# # Why? Because this alters the bias of the algorithm who will favor a higher weight to the categories like 4 and deprioritize levels like 1.\n\n# train_store_df = pd.get_dummies(train_store_df, columns = [\"Assortment\", \"StoreType\", \"PromoInterval\"], prefix = [\"is_Assortment\", \"is_StoreType\", \"is_PromoInterval\"])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c5f2727e00374e7b2ed66187bbf5c32318507cbc"
      },
      "cell_type": "code",
      "source": "# test_df = pd.read_csv(\"../input/test.csv\", parse_dates = [\"Date\"])\n# print(\"The Test Dataset has\", test_df.shape[0], \"observations and\", test_df.shape[1], \"variables\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "725cfbedc94cd23076c27bdceb96129c9e346a16"
      },
      "cell_type": "code",
      "source": "# # We want to make sure that we consider all events, so lets join the test dataset with the store set.\n# test_store_df = pd.merge(test_df, store_df, how = \"left\", on = \"Store\")\n\n# print(\"Now we have\", test_store_df.shape[0], \"observations and\", test_store_df.shape[1], \"columns\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "88e789c0ff3d98fe41d22043614537065c89461c"
      },
      "cell_type": "code",
      "source": "# # Lets create and conver the variables. Just like we did with the training dataset.\n\n# test_store_df[\"Month\"] = test_store_df.Date.dt.month\n# test_store_df[\"Year\"] = test_store_df.Date.dt.year\n# test_store_df[\"Day\"] = test_store_df.Date.dt.day\n\n# test_store_df[\"StateHoliday\"] = test_store_df[\"StateHoliday\"].astype(\"category\")\n# test_store_df[\"PromoInterval\"] = test_store_df[\"SchoolHoliday\"].astype(\"category\")\n# test_store_df[\"StoreType\"] = test_store_df[\"StoreType\"].astype(\"category\")\n# test_store_df[\"Assortment\"] = test_store_df[\"Assortment\"].astype(\"category\")\n\n# test_store_df[\"StateHoliday_cat\"] = test_store_df[\"StateHoliday\"].cat.codes\n# test_store_df[\"PromoInterval_cat\"] = test_store_df[\"PromoInterval\"].cat.codes\n# test_store_df[\"StoreType_cat\"] = test_store_df[\"StoreType\"].cat.codes\n# test_store_df[\"Assortment_cat\"] = test_store_df[\"Assortment\"].cat.codes\n\n# test_store_df[\"StateHoliday_cat\"] = test_store_df[\"StateHoliday_cat\"].astype(\"float\")\n# test_store_df[\"PromoInterval_cat\"] = test_store_df[\"PromoInterval_cat\"].astype(\"float\")\n# test_store_df[\"StoreType_cat\"] = test_store_df[\"StoreType_cat\"].astype(\"float\")\n# test_store_df[\"Assortment_cat\"] = test_store_df[\"Assortment_cat\"].astype(\"float\")\n\n# test_store_df[\"CompetitionOpenSince\"] = np.where((test_store_df[\"CompetitionOpenSinceMonth\"] == 0) & (test_store_df[\"CompetitionOpenSinceYear\"] == 0), \n#                                                 0,(test_store_df.Month - test_store_df.CompetitionOpenSinceMonth) + (12 *(test_store_df.Year - test_store_df.CompetitionOpenSinceYear)))\n\n# test_store_df[\"StateHoliday_cat\"] = test_store_df[\"StateHoliday\"].map({0:0, \"0\": 0, \"a\": 1, \"b\": 1, \"c\": 1})\n\n# test_store_df = test_store_df.drop([\"Date\", \"StateHoliday\", \"StoreType\", \"Assortment\", \"CompetitionOpenSinceMonth\", \"CompetitionOpenSinceYear\", \"PromoInterval\"], axis = 1)\n\n# # test_store_df = test_store_df.sort_index(axis = 1).reset_index(\"Id\") #make the ID variabe as index.\n\n# print(\"So we have in our training set\", train_store_df.shape[1], \"variables and in our testing set\", test_store_df.shape[1])\n# print(\"in the testing set, we are missing only 3 and this are: Sales, Customer and Avg Sales per Customer\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4e7bbffe490089d469d04b983f3abde808377f81"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dc4979f4d7a944d9ad8cb9c12524ad6fbece1ff7"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}